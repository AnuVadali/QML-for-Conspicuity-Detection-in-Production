{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":123140,"sourceType":"datasetVersion","datasetId":63362}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Conspicuity!!!","metadata":{}},{"cell_type":"code","source":"# Installations\n!pip install qiskit\n!pip install pennylane\n!pip install pennylane-lightning[kokkos]\n# !pip install pennylane-lightning\n# !pip install pennylane-lightning[gpu]","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-10T00:16:33.654767Z","iopub.execute_input":"2024-08-10T00:16:33.655053Z","iopub.status.idle":"2024-08-10T00:17:24.852251Z","shell.execute_reply.started":"2024-08-10T00:16:33.655028Z","shell.execute_reply":"2024-08-10T00:17:24.851307Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting qiskit\n  Downloading qiskit-1.1.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting rustworkx>=0.14.0 (from qiskit)\n  Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\nRequirement already satisfied: numpy<3,>=1.17 in /opt/conda/lib/python3.10/site-packages (from qiskit) (1.26.4)\nRequirement already satisfied: scipy>=1.5 in /opt/conda/lib/python3.10/site-packages (from qiskit) (1.11.4)\nRequirement already satisfied: sympy>=1.3 in /opt/conda/lib/python3.10/site-packages (from qiskit) (1.13.0)\nRequirement already satisfied: dill>=0.3 in /opt/conda/lib/python3.10/site-packages (from qiskit) (0.3.8)\nRequirement already satisfied: python-dateutil>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from qiskit) (2.9.0.post0)\nCollecting stevedore>=3.0.0 (from qiskit)\n  Downloading stevedore-5.2.0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from qiskit) (4.9.0)\nCollecting symengine>=0.11 (from qiskit)\n  Downloading symengine-0.11.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (1.2 kB)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.0->qiskit) (1.16.0)\nCollecting pbr!=2.1.0,>=2.0.0 (from stevedore>=3.0.0->qiskit)\n  Downloading pbr-6.0.0-py2.py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy>=1.3->qiskit) (1.3.0)\nDownloading qiskit-1.1.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading stevedore-5.2.0-py3-none-any.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading symengine-0.11.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (39.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.4/39.4 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pbr-6.0.0-py2.py3-none-any.whl (107 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.5/107.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: symengine, rustworkx, pbr, stevedore, qiskit\nSuccessfully installed pbr-6.0.0 qiskit-1.1.2 rustworkx-0.15.1 stevedore-5.2.0 symengine-0.11.0\nCollecting pennylane\n  Downloading PennyLane-0.37.0-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: numpy<2.0 in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.11.4)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from pennylane) (3.2.1)\nRequirement already satisfied: rustworkx in /opt/conda/lib/python3.10/site-packages (from pennylane) (0.15.1)\nCollecting autograd (from pennylane)\n  Downloading autograd-1.6.2-py3-none-any.whl.metadata (706 bytes)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from pennylane) (0.10.2)\nRequirement already satisfied: appdirs in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.4.4)\nCollecting semantic-version>=2.7 (from pennylane)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting autoray>=0.6.11 (from pennylane)\n  Downloading autoray-0.6.12-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: cachetools in /opt/conda/lib/python3.10/site-packages (from pennylane) (4.2.4)\nCollecting pennylane-lightning>=0.37 (from pennylane)\n  Downloading PennyLane_Lightning-0.37.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (23 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from pennylane) (2.32.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from pennylane) (4.9.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from pennylane) (21.3)\nRequirement already satisfied: future>=0.15.2 in /opt/conda/lib/python3.10/site-packages (from autograd->pennylane) (1.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->pennylane) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (2024.7.4)\nDownloading PennyLane-0.37.0-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading autoray-0.6.12-py3-none-any.whl (50 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading PennyLane_Lightning-0.37.0-cp310-cp310-manylinux_2_28_x86_64.whl (15.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading autograd-1.6.2-py3-none-any.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: semantic-version, autoray, autograd, pennylane-lightning, pennylane\nSuccessfully installed autograd-1.6.2 autoray-0.6.12 pennylane-0.37.0 pennylane-lightning-0.37.0 semantic-version-2.10.0\nRequirement already satisfied: pennylane-lightning[kokkos] in /opt/conda/lib/python3.10/site-packages (0.37.0)\nRequirement already satisfied: pennylane>=0.36 in /opt/conda/lib/python3.10/site-packages (from pennylane-lightning[kokkos]) (0.37.0)\nCollecting pennylane-lightning-kokkos (from pennylane-lightning[kokkos])\n  Downloading PennyLane_Lightning_Kokkos-0.37.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (23 kB)\nRequirement already satisfied: numpy<2.0 in /opt/conda/lib/python3.10/site-packages (from pennylane>=0.36->pennylane-lightning[kokkos]) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from pennylane>=0.36->pennylane-lightning[kokkos]) (1.11.4)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from pennylane>=0.36->pennylane-lightning[kokkos]) (3.2.1)\nRequirement already satisfied: rustworkx in /opt/conda/lib/python3.10/site-packages (from pennylane>=0.36->pennylane-lightning[kokkos]) (0.15.1)\nRequirement already satisfied: autograd in /opt/conda/lib/python3.10/site-packages (from pennylane>=0.36->pennylane-lightning[kokkos]) (1.6.2)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from pennylane>=0.36->pennylane-lightning[kokkos]) (0.10.2)\nRequirement already satisfied: appdirs in /opt/conda/lib/python3.10/site-packages (from pennylane>=0.36->pennylane-lightning[kokkos]) (1.4.4)\nRequirement already satisfied: semantic-version>=2.7 in /opt/conda/lib/python3.10/site-packages (from pennylane>=0.36->pennylane-lightning[kokkos]) (2.10.0)\nRequirement already satisfied: autoray>=0.6.11 in /opt/conda/lib/python3.10/site-packages (from pennylane>=0.36->pennylane-lightning[kokkos]) (0.6.12)\nRequirement already satisfied: cachetools in /opt/conda/lib/python3.10/site-packages (from pennylane>=0.36->pennylane-lightning[kokkos]) (4.2.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from pennylane>=0.36->pennylane-lightning[kokkos]) (2.32.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from pennylane>=0.36->pennylane-lightning[kokkos]) (4.9.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from pennylane>=0.36->pennylane-lightning[kokkos]) (21.3)\nRequirement already satisfied: future>=0.15.2 in /opt/conda/lib/python3.10/site-packages (from autograd->pennylane>=0.36->pennylane-lightning[kokkos]) (1.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->pennylane>=0.36->pennylane-lightning[kokkos]) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane>=0.36->pennylane-lightning[kokkos]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane>=0.36->pennylane-lightning[kokkos]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane>=0.36->pennylane-lightning[kokkos]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane>=0.36->pennylane-lightning[kokkos]) (2024.7.4)\nDownloading PennyLane_Lightning_Kokkos-0.37.0-cp310-cp310-manylinux_2_28_x86_64.whl (11.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pennylane-lightning-kokkos\nSuccessfully installed pennylane-lightning-kokkos-0.37.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Imports and Configs","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport json\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nimport random\n\nimport torch\nimport torch.nn as nn\n# import torch.nn.functional as F\n# import torch.optim as optim\n# from torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nimport pennylane.numpy as np\nimport pennylane as qml\nfrom pennylane.templates import RandomLayers\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay","metadata":{"execution":{"iopub.status.busy":"2024-08-10T00:17:24.854568Z","iopub.execute_input":"2024-08-10T00:17:24.855036Z","iopub.status.idle":"2024-08-10T00:17:32.642420Z","shell.execute_reply.started":"2024-08-10T00:17:24.854999Z","shell.execute_reply":"2024-08-10T00:17:32.641418Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Path defs and Data fetching","metadata":{}},{"cell_type":"code","source":"# Data Configs\nDATAPATH = Path(\"/kaggle/input/tig-aluminium-5083/al5083\")\nOUTPATH = Path(\"/kaggle/working/\")\nBATCH_SIZE = 1","metadata":{"execution":{"iopub.status.busy":"2024-08-10T00:17:32.643612Z","iopub.execute_input":"2024-08-10T00:17:32.644163Z","iopub.status.idle":"2024-08-10T00:17:32.648986Z","shell.execute_reply.started":"2024-08-10T00:17:32.644135Z","shell.execute_reply":"2024-08-10T00:17:32.647770Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Defining PyTorch compatible dataset for efficient dataloading\nclass TIGDataset(Dataset):\n    def __init__(self, datapath, preprocess: bool = True, datatype: str = \"train\", penny=True):\n        self.preprocess = preprocess\n        self.datapath = datapath\n        self.datatype = datatype\n        self.penny = penny\n        \n        with open (datapath/self.datatype/f\"{self.datatype}.json\", \"r\") as f:\n            self.metadata = json.load(f)\n        \n        self.data = [(k, v) for k, v in self.metadata.items()]\n    \n    def __len__(self):\n        return len(self.metadata)\n    \n    def __getitem__(self, idx):\n        image_path, label = self.data[idx]\n        image = Image.open(self.datapath/self.datatype/image_path)\n        if self.preprocess:  # Convert to grayscale, normalize, then resize to lower dimension\n            image = normalize(image.convert('L').resize((112, 112)))\n        if self.penny:\n            image = np.array(image).squeeze()\n        else:\n            image = torch.tensor(image, dtype=torch.float32)\n        return image, label\n    ","metadata":{"execution":{"iopub.status.busy":"2024-08-10T00:17:32.651163Z","iopub.execute_input":"2024-08-10T00:17:32.651437Z","iopub.status.idle":"2024-08-10T00:17:32.663989Z","shell.execute_reply.started":"2024-08-10T00:17:32.651414Z","shell.execute_reply":"2024-08-10T00:17:32.663136Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Defining train and test dataloaders\n\n# Pennylane format\ntrainset_penny = TIGDataset(datapath=DATAPATH, preprocess=True, datatype=\"train\", penny=True)\ntestset_penny = TIGDataset(datapath=DATAPATH, preprocess=True, datatype=\"test\", penny=True)\n\ntrainloader_penny = DataLoader(trainset_penny, batch_size=BATCH_SIZE, shuffle=True)\ntestloader_penny = DataLoader(testset_penny, batch_size=BATCH_SIZE, shuffle=False)\n\n# PyTorch format\ntrainset_torch = TIGDataset(datapath=DATAPATH, preprocess=True, datatype=\"train\", penny=False)\ntestset_torch = TIGDataset(datapath=DATAPATH, preprocess=True, datatype=\"test\", penny=False)\n\ntrainloader_torch = DataLoader(trainset_torch, batch_size=BATCH_SIZE, shuffle=True)\ntestloader_torch = DataLoader(testset_torch, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T00:17:32.665075Z","iopub.execute_input":"2024-08-10T00:17:32.665426Z","iopub.status.idle":"2024-08-10T00:17:32.767543Z","shell.execute_reply.started":"2024-08-10T00:17:32.665396Z","shell.execute_reply":"2024-08-10T00:17:32.766801Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def ppmetrics(y_true, y_pred, pname=\"classical\"):\n    # function to print and plot metrics\n    retval = (accuracy_score(y_true, y_pred), classification_report(y_true, y_pred), confusion_matrix(y_true, y_pred))\n    \n    print(\"\".join([\"=\"]*20) + f\" Metrics for {pname} \" + \"\".join([\"=\"]*20))\n    print(\"\\n\\n\")\n    \n    print(\"\".join([\"*\"]*10) + f\" Accuracy Score \" + \"\".join([\"*\"]*10))\n    print(\"\\n\")\n    print(f\"Accuracy: {retval[0]}\")\n    print(\"\\n\")\n    print(\"\".join([\"*\"]*15) + \"\".join([\"*\"]*15))\n    \n    print(\"\\n\\n\")\n    \n    print(\"\".join([\"*\"]*10) + f\" Classification report \" + \"\".join([\"*\"]*10))\n    print(\"\\n\")\n    print(retval[1])\n    print(\"\\n\")\n    print(\"\".join([\"*\"]*15) + \"\".join([\"*\"]*15))\n    \n    print(\"\\n\\n\")\n    \n    print(\"\".join([\"*\"]*10) + f\" Confusion Matrix \" + \"\".join([\"*\"]*10))\n    print(\"\\n\")\n    cmp = ConfusionMatrixDisplay(retval[2])\n    cmp.plot()\n    plt.show()\n    print(\"\\n\")\n    print(\"\".join([\"*\"]*15) + \"\".join([\"*\"]*15))\n    return retval","metadata":{"execution":{"iopub.status.busy":"2024-08-10T00:17:32.768539Z","iopub.execute_input":"2024-08-10T00:17:32.768829Z","iopub.status.idle":"2024-08-10T00:17:32.780729Z","shell.execute_reply.started":"2024-08-10T00:17:32.768806Z","shell.execute_reply":"2024-08-10T00:17:32.779834Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Classical models","metadata":{}},{"cell_type":"code","source":"# Defining CNN Encoder model with random weights\nclass ConvEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(\n            # Conv Layer 1 - 112x112x1\n            nn.Conv2d(in_channels=1, out_channels=4, kernel_size=2, stride=2),\n#             nn.BatchNorm1d(4),\n            nn.InstanceNorm1d(4),\n            nn.ReLU(),\n            \n            # Conv Layer 2 - 56x56x4\n            nn.Conv2d(in_channels=4, out_channels=16, kernel_size=2, stride=2),\n#             nn.BatchNorm1d(16),\n            nn.InstanceNorm1d(16),\n            nn.ReLU(),\n            \n            # Conv Layer 3 - 28x28x16\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=2, stride=2),\n#             nn.BatchNorm1d(32),\n            nn.InstanceNorm1d(32),\n            nn.ReLU(),\n            \n            # Conv Layer 4 - 14x14x32\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=2),\n#             nn.BatchNorm1d(64),\n            nn.InstanceNorm1d(64),\n            nn.ReLU(),\n        )\n        \n        self.linear = nn.Sequential(\n            # Linear Layer 1 - 7x7x64\n            nn.Linear(in_features=7*7*64, out_features=700),\n            nn.ReLU(),\n            \n            # Linear Layer 2 - 700\n            nn.Linear(in_features=700, out_features=100),\n            nn.ReLU(),\n            \n            # Linear Layer 3 - 100\n            nn.Linear(in_features=100, out_features=10),\n            nn.ReLU(),\n        )\n    \n        self.linear.apply(self.init_weights)\n    \n    def init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            torch.nn.init.xavier_uniform_(m.weight)\n            m.bias.data.fill_(0.01)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        x = x.view(-1)\n        out = self.linear(x)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-08-10T00:17:32.781851Z","iopub.execute_input":"2024-08-10T00:17:32.782098Z","iopub.status.idle":"2024-08-10T00:17:32.794683Z","shell.execute_reply.started":"2024-08-10T00:17:32.782077Z","shell.execute_reply":"2024-08-10T00:17:32.793956Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Function to encode all data into features for KNN\ndef encode_features(model, dataloader, force_cpu=False):\n    encoded_data = []\n    if force_cpu:\n        device = torch.device(\"cpu\")\n    else:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    model.eval()\n    for i, (X, y) in tqdm(enumerate(dataloader)):\n        with torch.no_grad():\n            X = X.to(device)\n            X_encoded = list(model(X).cpu().detach().numpy())\n            cols = [f\"X{i}\" for i in range(len(X_encoded))] + [\"y\"]\n            datarow = dict(zip(cols, X_encoded + list(y.numpy())))\n            encoded_data.append(datarow)\n    return pd.DataFrame(encoded_data)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T00:17:32.795557Z","iopub.execute_input":"2024-08-10T00:17:32.795824Z","iopub.status.idle":"2024-08-10T00:17:32.809085Z","shell.execute_reply.started":"2024-08-10T00:17:32.795769Z","shell.execute_reply":"2024-08-10T00:17:32.808327Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# # Encoding training images and testing images into features for KNN without training\n# train_df = encode_features(ConvEncoder(), trainloader_torch, force_cpu=False)\n# test_df = encode_features(ConvEncoder(), testloader_torch, force_cpu=False)\n\n# train_df.to_csv(OUTPATH / \"train_classical.csv\", index=False)\n# test_df.to_csv(OUTPATH / \"test_classical.csv\", index=False)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-09T22:49:20.843890Z","iopub.execute_input":"2024-08-09T22:49:20.844633Z","iopub.status.idle":"2024-08-09T22:49:20.848771Z","shell.execute_reply.started":"2024-08-09T22:49:20.844600Z","shell.execute_reply":"2024-08-09T22:49:20.847788Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Loading dataframes\ntrain_df = pd.read_csv(OUTPATH / \"train_classical.csv\")\ntest_df = pd.read_csv(OUTPATH / \"test_classical.csv\")\n\ndrop_cols_train = [col for col in train_df.columns if col.startswith(\"Unnamed\")]\ndrop_cols_test = [col for col in test_df.columns if col.startswith(\"Unnamed\")]\n\ntrain_df.drop(drop_cols_train, axis=1, inplace=True)\ntest_df.drop(drop_cols_test, axis=1, inplace=True)\n\n# train_df = train_df.apply(lambda x: eval(x).item())\n# test_df = test_df.apply(lambda x: eval(x).item())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training KNN on classical data\nX, y = train_df.drop(\"y\", axis=1).to_numpy(), train_df[\"y\"].to_numpy()\n\nknn_classical = KNeighborsClassifier(n_neighbors=7)\nknn_classical.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T22:49:21.977976Z","iopub.execute_input":"2024-08-09T22:49:21.978744Z","iopub.status.idle":"2024-08-09T22:49:22.021210Z","shell.execute_reply.started":"2024-08-09T22:49:21.978711Z","shell.execute_reply":"2024-08-09T22:49:22.020335Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"KNeighborsClassifier(n_neighbors=7)","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=7)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=7)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"# Testing KNN with classical encoding of features\nX_test, y_test = test_df.drop(\"y\", axis=1).to_numpy(), test_df[\"y\"].to_numpy()\ny_pred = knn_classical.predict(X_test)\n\nacc, cr, cm = ppmetrics(y_test, y_pred, \"classical\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-09T22:49:22.587570Z","iopub.execute_input":"2024-08-09T22:49:22.588208Z","iopub.status.idle":"2024-08-09T22:49:23.836982Z","shell.execute_reply.started":"2024-08-09T22:49:22.588166Z","shell.execute_reply":"2024-08-09T22:49:23.836143Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"==================== Metrics for classical ====================\n\n\n\n********** Accuracy Score **********\n\n\nAccuracy: 0.24468731026108076\n\n\n******************************\n\n\n\n********** Classification report **********\n\n\n              precision    recall  f1-score   support\n\n           0       0.37      0.36      0.37      2189\n           1       0.00      0.00      0.00       351\n           2       0.27      0.32      0.29      2078\n           3       0.00      0.00      0.00      1007\n           4       0.11      0.20      0.15       729\n           5       0.00      0.01      0.00       234\n\n    accuracy                           0.24      6588\n   macro avg       0.13      0.15      0.14      6588\nweighted avg       0.22      0.24      0.23      6588\n\n\n\n******************************\n\n\n\n********** Confusion Matrix **********\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"\n\n******************************\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Quantum models\n- We use Quanvolution to encode the data into features here","metadata":{}},{"cell_type":"code","source":"dev = qml.device(\"lightning.kokkos\", wires=4)\nn_layers = 1  # Number of random layers\nrand_params = np.random.uniform(high=2 * np.pi, size=(n_layers, 4))\nimage_size = (112, 112)\n\n@qml.qnode(dev)\ndef circuit(phi):\n    for j in range(4):\n        qml.RY(np.pi * phi[j], wires=j)\n\n    # Random quantum circuit\n    RandomLayers(rand_params, wires=list(range(4)))\n    # Measurement producing 4 classical output values\n    return [qml.expval(qml.PauliZ(j)) for j in range(4)]\n\ndef quanv(arr):\n    out = np.zeros((int(arr.shape[0]/2), int(arr.shape[1]/2), 4))\n\n    # Loop over the coordinates of the top-left pixel of 2X2 squares\n    for j in range(0, arr.shape[0], 2):\n        for k in range(0, arr.shape[1], 2):\n            # Process a squared 2x2 region of the image with a quantum circuit\n            q_results = circuit(\n                [\n                    arr[j, k,],\n                    arr[j, k + 1],\n                    arr[j + 1, k],\n                    arr[j + 1, k + 1]\n                ]\n            )\n            # Assign expectation values to different channels of the output pixel (j/2, k/2)\n            for c in range(4):\n                out[j // 2, k // 2, c] = q_results[c]\n    return out","metadata":{"execution":{"iopub.status.busy":"2024-08-10T00:22:56.408216Z","iopub.execute_input":"2024-08-10T00:22:56.408817Z","iopub.status.idle":"2024-08-10T00:22:56.419889Z","shell.execute_reply.started":"2024-08-10T00:22:56.408764Z","shell.execute_reply":"2024-08-10T00:22:56.419086Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# class QConvLayer():\n#     def __init__(self, in_channels, out_channels, image_size):\n#         self.in_channels = in_channels\n#         self.out_channels = out_channels\n#         self.image_size = image_size\n#         self.n_layers = 1    # Number of random layers\n#         self.rand_params = np.random.uniform(high=2 * np.pi, size=(self.n_layers, out_channels))\n        \n#     @qml.qnode(dev)\n#     def circuit(self, phi):\n#         for j in range(self.out_channels):\n#             qml.RY(np.pi * phi[j], wires=j)\n\n#         # Random quantum circuit\n#         RandomLayers(self.rand_params, wires=list(range(self.out_channels)))\n#         # Measurement producing 4 classical output values\n#         return [qml.expval(qml.PauliZ(j)) for j in range(self.out_channels)]\n    \n#     def quanv(self, arr):\n#         out = np.zeros((int(arr.shape[0]/2), int(arr.shape[1]/2), self.out_channels))\n\n#         # Loop over the coordinates of the top-left pixel of 2X2 squares\n#         for j in range(0, arr.shape[0], 2):\n#             for k in range(0, arr.shape[1], 2):\n#                 # Process a squared 2x2 region of the image with a quantum circuit\n#                 q_results = self.circuit(\n#                     [\n#                         arr[j, k,],\n#                         arr[j, k + 1],\n#                         arr[j + 1, k],\n#                         arr[j + 1, k + 1]\n#                     ]\n#                 )\n#                 # Assign expectation values to different channels of the output pixel (j/2, k/2)\n#                 for c in range(self.out_channels):\n#                     out[j // 2, k // 2, c] = q_results[c]\n#         return out\n\n    \n# Defining Quantum CNN Encoder model with random weights - One Conv Layer + remaining classical layers\n\nclass QConvEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n#         self.qconv = QConvLayer(1, 4, (112, 112))\n\n        self.conv = nn.Sequential(\n            # Conv Layer 1 - 56x56x4\n            nn.Conv2d(in_channels=4, out_channels=16, kernel_size=2, stride=2),\n#             nn.BatchNorm1d(16),\n            nn.InstanceNorm1d(16),\n            nn.ReLU(),\n            \n            # Conv Layer 2 - 28x28x16\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=2, stride=2),\n#             nn.BatchNorm1d(32),\n            nn.InstanceNorm1d(32),\n            nn.ReLU(),\n            \n            # Conv Layer 3 - 14x14x32\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=2),\n#             nn.BatchNorm1d(64),\n            nn.InstanceNorm1d(64),\n            nn.ReLU(),\n        )\n        \n        self.linear = nn.Sequential(\n            # Linear Layer 1 - 7x7x64\n            nn.Linear(in_features=7*7*64, out_features=700),\n            nn.ReLU(),\n            \n            # Linear Layer 2 - 700\n            nn.Linear(in_features=700, out_features=100),\n            nn.ReLU(),\n            \n            # Linear Layer 3 - 100\n            nn.Linear(in_features=100, out_features=10),\n            nn.ReLU(),\n        )\n    \n        self.linear.apply(self.init_weights)\n    \n    def init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            torch.nn.init.xavier_uniform_(m.weight)\n            m.bias.data.fill_(0.01)\n    \n    def forward(self, x):\n        x = quanv(x.squeeze())\n        x = torch.tensor(x, dtype=torch.float32).view(4, 56, 56)\n        x = self.conv(x)\n        x = x.view(-1)\n        out = self.linear(x)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-08-10T00:23:00.059472Z","iopub.execute_input":"2024-08-10T00:23:00.060277Z","iopub.status.idle":"2024-08-10T00:23:00.073253Z","shell.execute_reply.started":"2024-08-10T00:23:00.060244Z","shell.execute_reply":"2024-08-10T00:23:00.072362Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Encoding training images and testing images into features for KNN without training\ntrain_df_Q = encode_features(QConvEncoder(), trainloader_penny, force_cpu=True)\ntest_df_Q = encode_features(QConvEncoder(), testloader_penny, force_cpu=True)\n\ntrain_df_Q.to_csv(OUTPATH / \"train_quantum.csv\", index=False)\ntest_df_Q.to_csv(OUTPATH / \"test_quantum.csv\", index=False)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-10T00:23:00.967928Z","iopub.execute_input":"2024-08-10T00:23:00.968591Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"798b33938b094d808c2d230c55777789"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n/opt/conda/lib/python3.10/site-packages/pennylane/math/utils.py:228: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loading dataframes\ntrain_df_Q = pd.read_csv(OUTPATH / \"train_quantum.csv\")\ntest_df_Q = pd.read_csv(OUTPATH / \"test_quantum.csv\")\n\ndrop_cols_train = [col for col in train_df_Q.columns if col.startswith(\"Unnamed\")]\ndrop_cols_test = [col for col in test_df_Q.columns if col.startswith(\"Unnamed\")]\n\ntrain_df_Q.drop(drop_cols_train, axis=1, inplace=True)\ntest_df_Q.drop(drop_cols_test, axis=1, inplace=True)\n\n# train_df = train_df.apply(lambda x: eval(x).item())\n# test_df = test_df.apply(lambda x: eval(x).item())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training KNN on quantum data\nXq, yq = train_df_Q.drop(\"y\", axis=1).to_numpy(), train_df_Q[\"y\"].to_numpy()\n\nknn_quantum = KNeighborsClassifier(n_neighbors=7)\nknn_quantum.fit(Xq, yq)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing KNN with quantum encoding of features\nX_test_Q, y_test_Q = test_df_Q.drop(\"y\", axis=1).to_numpy(), test_df_Q[\"y\"].to_numpy()\ny_pred_Q = knn_quantum.predict(X_test_Q)\n\naccQ, crQ, cmQ = ppmetrics(y_test_Q, y_pred_Q, \"quantum\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}